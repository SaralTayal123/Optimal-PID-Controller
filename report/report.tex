\documentclass[11pt]{article}

\input{sammath.sty}

\newcommand{\mycourse}{18-660: Optimization}
\newcommand{\secnum}{A}
\newcommand{\myhwnum}{}

\renewcommand{\assignmenttype}{Final Report}
\renewcommand{\partnername}{Saral Tayal}
\renewcommand{\partnerandrew}{\textit{stayal}}

\usepackage[colorlinks=true,citecolor=magenta]{hyperref}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Document begins here %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\graphicspath{{./img/}}

\begin{document}
%    \headings
    \headingsshared
    
    \section{Introduction}
    We are entering a new age of robotics, where robots are becoming more complex (e.g. Boston Dynamics), and more popular and commonplace (e.g. Roomba/self-driving cars). This increased complexity and computation has given rise to a new field of robot control: Optimal Control. By leveraging simplified robot dynamics, Optimal Control allows us to create complex robot trajectories and perform very effective feedback control to external disturbances (e.g. gusts of wind for a quadrotor, or a humanoid robot slipping on pavement). 
    
    Within Optimal Control, LQR (Linear Quadratic Regulators) are one of the most popular controllers due to their simplicity, robustness guarantees, and ease of implementation on actual hardware. Within the domain of LQR, there are 2 extremely popular controllers: ILQR and TVLQR. ILQR is a simple controller that simply figures out the feedback gain matrix at a robot’s linearized equilibrium point. This is extremely simple and memory efficient but only allows for simple trajectories due to the singular linearization at the equilibrium. TVLQR tackles that shortcoming by linearizing at multiple points over an entire trajectory and storing feedback gains at each of these points. While this allows for significantly more complex trajectories, it comes at the cost of high memory usage that scales directly with the linearization resolution and trajectory length. \cite{underactuated}
    
    Our research involves bridging the gap between these 2 popular LQR-based control techniques. We wish to retain the memory efficiency of ILQR while incorporating elements of TVLQR’s trajectory-awareness into our optimization problem.
    
    
    \section{Problem Formulation} \label{sec:prob}
    \subsection{Control System} \label{sec:prob:control}
    For our simulations, we are modelling the dynamics of the car with a simple non-linear bicycle model, with the following state ($x$) and control ($u$)
%    \begin{align*} 
%        x = \begin{bmatrix}
%            p_x \\ p_y \\ \theta \\ \delta \\ v \\ \omega
%        \end{bmatrix} &&
%        u = \begin{bmatrix}
%            a \\ \dot{\delta}
%        \end{bmatrix}
%    \end{align*}
    \begin{align*} 
        x = \transbmat{p_x & p_y & \theta & \delta & v} &&
        u = \transbmat{a & \dot{\delta}}
    \end{align*}
    where $p_x, p_y$ is the position, $\theta$ is the orientation, $\delta$ is the steering angle, and $v$ is the velocity. The controls for the bike are acceleration $a$, and steering angle rate $\dot{\delta}$.
    
    In order to make the system control-affine for easy optimization, we linearize our approximate dynamics model about $X_{ref}$ and $U_{ref}$ to get the following Jacobians for each step $k \in [1, N] $ in the trajectory:
    \begin{align*}
        A_k = \frac{\partial f}{\partial x}\bigg|_{x_{ref,k},u_{ref,k}} \in \R^{5\times 5}&& 
        B_k = \frac{\partial f}{\partial u}\bigg|_{x_{ref,k},u_{ref,k}}  \in \R^{5\times 2}
    \end{align*}
    where $f(x,u)$ is our approximate discrete dynamics model, $X_{ref}$ is the optimal trajectory computed offline with approximate dynamics model, and $U_{ref}$ is the optimal controls computed offline with approximate dynamics model. \\
%    - $X_{sim}$ (`Xsim`) -  Simulated trajectory with real dynamics model.
    With this, the system becomes control-affine, where the states are given by
    \begin{align*}
        x_{k+1} = A_k x_k + B_k u_k
    \end{align*}
    and the proportional-derivative (PD) controller gives the control
    \begin{align*}
        u_k = - \left[\strut P * \left(x_{k} - x_{ref,k}\right) + D (x_k - x_{k-1}) \strut \right]
    \end{align*}
    where $P,D \in \R^{2\times 5}$ are the proportional and derivative control matrices to be found by our algorithm.
    
    \subsection{Optimizing for PD matrices} \label{sec:prob:pdoptim}
    In order to automatically tune the PD matrices, we optimize for best PD matrices that minimize the quadratic cost of the trajectory generated by the PID controller, as constrained by control-affine system dynamics. Mathematically, we attempt to solve the following problem:
    \begin{align*}
        \min_{P,D}\qquad & \mathrm{Cost}(X, X_{ref}) \fracln
        \st \quad & x_1 = x_{ref, 1} \vecln
        &         x_{k+1} = A_k x_k + B_k u_k \vecln
        &         u_k = - \left[\strut P * \left(x_{k} - x_{ref,k}\right) + D (x_k - x_{k-1}) \strut \right]
f    \end{align*}
    where $X, X_{ref}$ are the generated and reference trajectories respectively (a direct function of the states $x_k$), and $A_k, B_k$ are the Jacobians representing the linearized system dynamics as mentioned in Section \ref{sec:prob:control} above.
    
    \subsubsection{Without Regularization} \label{sec:prob:pdoptim:noreg}
    Our initial formulation without regularization involved a simple quadratic cost.
    \begin{align*}
        \mathcal{L}_Q = \mathrm{Cost}(X, X_{ref}) = \transp{X - X_{ref}} Q \left(X - X_{ref}\right)
    \end{align*}
    where $Q \curlygeq 0 $ is a quadratic cost matrix. Since the cost function is quadratic and the constraints are affine, we have a quadratic programming problem, which can be solved efficiently as noted in class.
    
    \subsection{With Regularization} \label{sec:prob:pdoptim:wreg}
    However, as noted in Section \ref{sec:results:singletraj:noreg}, the simple formulation results in exploding P and D values. To combat this, we add L1 regularization to the quadratic cost as follows:
    \begin{align*}
        \mathcal{L}_{Q,reg} = \mathrm{Cost}(X, X_{ref}) = \transp{X - X_{ref}} Q \left(X - X_{ref}\right) + \lambda \left(\strut \norm{P}_1 + \norm{D}_1 \strut \right)
    \end{align*}
    where $\lambda$ is a hyperparameter that affects the amount of regularization in the system. Although the L1 regularization makes the problem no longer quadratic, the problem is still convex and can be solved quite efficiently by proximal gradient descent.
    
    \section{Results} \label{sec:results}
    \subsection{Single trajectory} \label{sec:results:singletraj}
    Saral setup a simulation architecture in Julia with a preset trajectory to control a car with bicycle dynamics, while Samuel worked on the problem formulation and metrics. The code can be found at \href{https://github.com/SaralTayal123/OptimizationFinalProj}{https://github.com/SaralTayal123/OptimizationFinalProj}. We started with a single parabolic trajectory, shown in Figure \ref{fig:trajectory} below.
    
    \begin{figure}[h!]
        \centering
        \includegraphics[width=0.5\linewidth]{trajectory}
        \caption{Simple parabolic trajectory for robot to follow}
        \label{fig:trajectory}
    \end{figure}

    \subsubsection{Metrics} \label{sec:results:metrics}
%    \textbf{TODO: Include metrics here, with references}
    Borrowing from the robotics field of odometry \cite{metrics1}, two metrics based off the absolute root-mean-square error (RMSE) were included to gauge how well the optimizer performed in trajectory optimization:
    \pagebreak
    
    \begin{itemize}
        \item Absolute Position RMSE: RMSE positional difference between calculated and reference trajectory at each point \begin{align*}
            \mathrm{RMSE}_{\mathrm{pos}} &= \transp{X - X_{ref}} * \mathrm{diag}\transbmat{1 & 1 & 0 & 0 & 0} * \left(X - X_{ref}\right) \fracln
            &= \sqrt{\strut \dfrac{1}{N} \sum^N_{k=1} \left[\left(p_{x,k} - p_{xref, k}\right)^2 + \left(p_{y,k} - p_{yref, k}\right)^2\right]  \strut\strut }
        \end{align*}
        \item Absolute Orientation RMSE: RMSE orientational difference between calculated and reference trajectory at each point\begin{align*}
            \mathrm{RMSE}_{\mathrm{orient}} &= \transp{X - X_{ref}} *  \mathrm{diag}\transbmat{0 & 0 & 1 & 0 & 0} * \left(X - X_{ref}\right) \fracln
            &= \sqrt{\strut \dfrac{1}{N} \sum^N_{k=1} \left(\theta_{k} - \theta_{ref, k}\right)^2 \strut\strut }
        \end{align*}
    \end{itemize}
    
    We also included the quadratic trajectory cost without regularization outlined in Section as a metric. It is mainly useful for debugging and comparing which trajectories were the hardest to optimize from a cost standpoint.

    \def\quadcost{\ensuremath{\mathcal{L}_Q}}
    \def\rmsepos{\ensuremath{\mathrm{RMSE}_{\mathrm{pos}}}}
    \def\rmseorient{\ensuremath{\mathrm{RMSE}_{\mathrm{orient}}}}

    \subsubsection{Baseline: No Control Input}
    We ran the tracker with no controller input (i.e. $P = D = 0$) as a baseline. The generated trajectory is shown in Figure \ref{fig:trajectorynocontrol} below.
    
    \begin{figure}[h!]
        \centering
        \includegraphics[width=0.55\linewidth]{img/trajectory_nocontrol}
        \caption{Generated trajectory without control ($ P = D = 0$)}
        \label{fig:trajectorynocontrol}
    \end{figure}

    \pagebreak
    
    \subsubsection{Without Regularization} \label{sec:results:singletraj:noreg}
    With the simple non-regularized quadratic cost, we found that our P/D matrices had very large values, and caused the optimizer to fail completely, with infinite values for all metrics. See Section \ref{sec:discussion:failurecase} for a discussion on potential failure reasons.
        \begin{align*}
            \hspace{-2em}
            P &= \bmat{-25.48 & -43.88 & 228.95 & 904.98 & 1151.69 \\
                -1.81 & -4.19 & 29.26 & -70.38 & -26.2} \fracln 
            D &
            = \bmat{-130.93 & -71.37 & -1142.36 & 22.02 & 2424.28 \\
                -9.39 & 10.48  & -19.01 & 3.23 & 186.85}
        \end{align*}
    
    \begin{figure}[h!]
        \centering
        \includegraphics[width=0.6\linewidth]{img/trajectory_pd_noreg}
        \caption{Calculated trajectory with no regularization}
        \label{fig:trajectorypdnoreg}
    \end{figure}
    
    \pagebreak
    
    \subsubsection{With Regularization} \label{sec:results:singletraj:wreg}
    As can be seen in Figure \ref{fig:trajectorypdreg} below, regularization improves the generated trajectory signficantly and results in sparse $P $,$ D$ matrices with relatively small values, as expected from L1 regularization. 
        \begin{align*}
%            \hspace{-2em}
            P &= \bmat{-0.79 & 0.11 & 0 & 0 & 0 \\ -0.22 & -0.08 & 0 & 0 & 0} &
            D &= \bmat{-0.42 & 0 & 0 & 0 & 0 \\  0.2 & 0 & 0 & 0 & 0}
        \end{align*}
    
    \begin{figure}[h!]
        \centering
        \includegraphics[width=0.6\linewidth]{img/trajectory_pd_reg}
        \caption{Generated trajectory with regularization}
        \label{fig:trajectorypdreg}
    \end{figure}
    
    \subsubsection{Metrics Comparison} \label{sec:results:singletraj:metrics}
    The metrics for the above cases are shown in Table \ref{tab:metrics:compare} below.
    \begin{table}[h!]
        \centering
        \tabvpad{1.5}
        \tabhpad{1.2em}
        \caption{Comparison of Metrics (rounded to 3 d.p.)} \label{tab:metrics:compare}
        \begin{tabular}{c|ccc}
            \hline
            & Baseline & With Reg. & \textit{Without Reg.}
            \\ \hline
            \rmsepos & 7.491 & 0.290 & $\infty$ \\
            \rmseorient & 0.976 & 0.060 & $\infty$ \\
            \quadcost & 314554.656 & 54.695  & $\infty$ \\
            \hline
        \end{tabular}
    \end{table}
    
    As evident from Table \ref{tab:metrics:compare} above, the generated PD matrix using our algorithm (with regularization) clearly improved all metrics. The metrics for the optimization without regularization were all reported to be infinity, and we attempt to discuss why in Section \ref{sec:discussion:failurecase} below.
    
    \section{Discussion} \label{sec:discussion}
    \subsection{Potential Reasons why Non-Regularized Quadratic Cost Failed} \label{sec:discussion:failurecase}
    
    \subsection{Effect of Regularization} \label{sec:discussion:regularization}
    
    \section{Learning Objectives} \label{sec:learningobjectives}
    In this project, we originally set out to learn more about optimal control and how optimization can be applied to the field. 
    This project was a indeed good opportunity (especially for Samuel who was more new to the field) to learn more about optimal control theory, particularly dynamics and trajectory optimization. A bigger part of our learning process involved trying to formulate our optimization problem, using what we learnt in class. Since dynamics-aware and trajectory-aware control problems can be quite complicated, it was challenging to do so in a way that was intuitive, while staying convex and easily solvable. Originally, we had wanted to autotune Q/R matrices for optimal control, but quickly found that the formulation was too complex; eventually, we simplified the problem by focusing on PID control and autotuning (optimizing for) P/D matrices. Still, formulating the optimization problem from scratch was not exactly non-trivial, but we used the techniques learnt in class to do it. In particular, we first tried a quadratic cost function with linear constraints to formulate a simple quadratic programming problem, and when that cause the values to explode, we turned to L1 regularization, which worked very well. 
    
    \bibliographystyle{ieeetr}
    \bibliography{cite.bib}
\end{document}

